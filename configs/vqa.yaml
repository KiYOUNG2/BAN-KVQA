DataArguments:

  # DataPathArguments
  # dataset_name: ./data/aistage-mrc/train_dataset
  dataset_path: ./data
  # context_path: wiki_800.json
  # overwrite_cache: True
  preprocessing_num_workers: 10
  # make_mask: False
  # masking_type: "mask"
  # curriculum_split_name: train
  # curriculum_learn: False
  # dataset_version: v1.0.0
  
  # TokenizerArguments
  max_seq_length: 16
  pad_to_max_length: True
  # doc_stride: 128
  # max_answer_length: 30
  # return_token_type_ids: False

  # HightlightingArguments
  # do_underline: False
  # do_punctuation: False
  # top_k_punctuation: 10
  # punct_model_name_or_path: ./outputs/run_test
  # punct_use_auth_token: True
  # punct_revision: main
  # punct_max_seq_length : 100

  # RetrievalArguments
  # retrieval_mode: elastic_engine
  # retrieval_name: elastic_search
  # rebuilt_index: True
  # retrieval_tokenizer_name: mecab
  # sp_max_features: 50000
  # sp_ngram_range: [1,2]
  # top_k_retrieval: 10
  # use_faiss: False
  # eval_retrieval: True
  # num_clusters: 64
  
  # ElasticSearchArguments
  # index_name: wiki-index
  # stopword_path: user_dic/my_stop_dic.txt
  # decompound_mode: mixed
  # b: 0.5
  # k1: 1.3
  # es_host_address: localhost:9200
  # es_similarity: bm25_similarity
  # use_korean_stopwords: False
  # use_korean_synonyms: False
  # lowercase: False
  # nori_readingform: False
  # cjk_bigram: False
  # decimal_digit: False
  # dfr_basic_model: g
  # dfr_after_effect: l
  # es_normalization: h2
  # dfi_measure: standardized
  # ib_distribution: ll
  # ib_lambda: df
  # lmd_mu: 2000
  # lmjm_lambda: 0.1
  
  # DenoisingArguments
  # denoising_func: None
  # permute_sentence_ratio: 0.0

ModelArguments:

  # ModelArguments
  model_name_or_path: klue/roberta-base
  # reader_type: extractive
  architectures: bertrnn
  config_name: null
  tokenizer_name: null
  model_cache_dir: cache
  model_cache_dir: None
  model_init: ban
  # use_auth_token: True
  # revision: main
  
  # ModelHeadArguments
  model_head: bertrnn
  # qa_conv_out_channel: 1024
  # qa_conv_input_size: 384
  # qa_conv_n_layers: 5
  # use_auth_token: Fasle

  # VQAArguments
  num_classes: 2423
  v_dim: 2048
  num_hid: 768
  op: null
  gamma: 8
  finetune_q: False 
  on_do_q: False

TrainingArguments:

  # HfTrainingArguments
  report_to: wandb
  run_name: run_test
  output_dir: ./saved_models/ban-kvqa-bertrnn-fixsptoken-smoke3
  # overwrite_output_dir: False
  learning_rate: 1e-3
  do_train: True
  do_eval: True
  do_predict: True            # when True, must set eval_retrieval = True
  evaluation_strategy: steps  # when do_eval == False, set 'no'
  save_strategy: steps        # when do_eval == False, set 'no'
  per_device_train_batch_size: 64
  per_device_eval_batch_size: 128
  num_train_epochs: 20
  # do_pos_ensemble: False
  # eval_steps: 100
  # save_steps: 100
  # save_total_limit: 3
  # fp16: True
  # weight_decay: 0.01
  # warmup_steps: 200
  # load_best_model_at_end: True
  # metric_for_best_model: exact_match
  # logging_dir: logs
  # lr_scheduler_type: cosine

  # # Seq2SeqTrainingArguments
  # sortish_sampler: False
  # predict_with_generate: False
  # generation_max_length: null
  # generation_num_beams: null
  

ProjectArguments:
  
  # AnalyzerArguments
  wandb_project: mrc
  checkpoint: null